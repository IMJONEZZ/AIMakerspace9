{
  "version": "1",
  "metadata": {
    "marimo_version": "0.19.6"
  },
  "cells": [
    {
      "id": "MJUe",
      "code_hash": "f4352501d74cc62d72acef521ff84ab0",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h1 id=\"agentic-rag-from-scratch-building-with-langgraph-and-open-source-models\">Agentic RAG From Scratch: Building with LangGraph and Open-Source Models</h1>\n<span class=\"paragraph\">In this notebook, we'll look under the hood of <code>create_agent</code> and build an agentic RAG application <strong>from scratch</strong> using LangGraph's low-level primitives and locally-hosted open-source models.</span>\n<span class=\"paragraph\"><strong>Learning Objectives:</strong></span>\n<ul>\n<li>Understand LangGraph's core constructs: StateGraph, nodes, edges, and conditional routing</li>\n<li>Build a ReAct agent from scratch without high-level abstractions</li>\n<li>Use Ollama to run open-source models locally (gpt-oss:20b + embeddinggemma)</li>\n<li>Transition from <code>aimakerspace</code> utilities to the LangChain ecosystem</li>\n</ul>\n<h2 id=\"table-of-contents\">Table of Contents:</h2>\n<ul>\n<li>\n<span class=\"paragraph\"><strong>Breakout Room #1:</strong> LangGraph Fundamentals &amp; Building Agents from Scratch</span>\n<ul>\n<li>Task 1: Dependencies &amp; Ollama Setup</li>\n<li>Task 2: LangGraph Core Concepts (StateGraph, Nodes, Edges)</li>\n<li>Task 3: Building a ReAct Agent from Scratch</li>\n<li>Task 4: Adding Tools to Your Agent</li>\n<li>Question #1 &amp; Question #2</li>\n<li>Activity #1: Implement a Custom Routing Function</li>\n</ul>\n</li>\n<li>\n<span class=\"paragraph\"><strong>Breakout Room #2:</strong> Agentic RAG with Local Models</span>\n<ul>\n<li>Task 5: Loading &amp; Chunking with LangChain</li>\n<li>Task 6: Setting up Qdrant with Local Embeddings</li>\n<li>Task 7: Creating a RAG Tool</li>\n<li>Task 8: Building Agentic RAG from Scratch</li>\n<li>Question #3 &amp; Question #4</li>\n<li>Activity #2: Extend the Agent with Memory</li>\n</ul>\n</li>\n</ul></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "vblA",
      "code_hash": "7cf1798adf1605c47493c5f5ad66b894",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><hr />\n<h1 id=\"breakout-room-1\">Breakout Room #1</h1>\n<h2 id=\"langgraph-fundamentals-building-agents-from-scratch\">LangGraph Fundamentals &amp; Building Agents from Scratch</h2></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "bkHC",
      "code_hash": "f3e110bddd3276a76b7b1ebe6efa1897",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"task-1-dependencies-ollama-setup\">Task 1: Dependencies &amp; Ollama Setup</h2>\n<span class=\"paragraph\">Before we begin, make sure you have:</span>\n<ol>\n<li><strong>Ollama installed</strong> - Download from <a href=\"https://ollama.com/\" rel=\"noopener noreferrer\" target=\"_blank\">ollama.com</a></li>\n<li><strong>Ollama running</strong> - Start with <code>ollama serve</code> in a terminal</li>\n<li><strong>Models pulled</strong> - Run these commands:</li>\n</ol>\n<div class=\"language-bash codehilite\"><pre><span></span><code><span class=\"c1\"># Chat model for reasoning and generation (~12GB)</span>\nollama<span class=\"w\"> </span>pull<span class=\"w\"> </span>gpt-oss:20b\n\n<span class=\"c1\"># Embedding model for RAG (~622MB)</span>\nollama<span class=\"w\"> </span>pull<span class=\"w\"> </span>embeddinggemma\n</code></pre></div>\n<blockquote>\n<span class=\"paragraph\"><strong>Note</strong>: If you don't have enough RAM/VRAM for <code>gpt-oss:20b</code> (requires 16GB+ VRAM or 24GB+ RAM), you can substitute with <code>llama3.2:3b</code> or another smaller model.</span>\n</blockquote>\n<span class=\"paragraph\"><strong>\ud83d\udcda Documentation:</strong></span>\n<ul>\n<li><a href=\"https://ollama.com/download\" rel=\"noopener noreferrer\" target=\"_blank\">Ollama Installation Guide</a></li>\n<li><a href=\"https://ollama.com/library/gpt-oss\" rel=\"noopener noreferrer\" target=\"_blank\">gpt-oss Model Card</a></li>\n<li><a href=\"https://ollama.com/library/embeddinggemma\" rel=\"noopener noreferrer\" target=\"_blank\">EmbeddingGemma Model Card</a></li>\n<li><a href=\"https://python.langchain.com/docs/integrations/providers/ollama/\" rel=\"noopener noreferrer\" target=\"_blank\">langchain-ollama Integration</a></li>\n</ul></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "Xref",
      "code_hash": "cdb01155cbb835d7bb6967938b046180",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"task-2-langgraph-core-concepts\">Task 2: LangGraph Core Concepts</h2>\n<span class=\"paragraph\">In Session 3, we used <code>create_agent</code> which abstracts away the complexity. Now let's understand what's happening under the hood!</span>\n<h3 id=\"langgraph-models-workflows-as-graphs-with-three-key-components\">LangGraph models workflows as <strong>graphs</strong> with three key components:</h3>\n<h3 id=\"1-state\">1. State</h3>\n<span class=\"paragraph\">A shared data structure that represents the current snapshot of your application:</span>\n<div class=\"language-python codehilite\"><pre><span></span><code><span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">AgentState</span><span class=\"p\">(</span><span class=\"n\">TypedDict</span><span class=\"p\">):</span>\n    <span class=\"n\">messages</span><span class=\"p\">:</span> <span class=\"n\">Annotated</span><span class=\"p\">[</span><span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">add_messages</span><span class=\"p\">]</span>  <span class=\"c1\"># Conversation history</span>\n</code></pre></div>\n<span class=\"paragraph\">The <code>add_messages</code> <strong>reducer</strong> ensures new messages are appended (not replaced) when the state updates.</span>\n<h3 id=\"2-nodes\">2. Nodes</h3>\n<span class=\"paragraph\">Python functions that encode the logic of your agent:</span>\n<ul>\n<li>Receive the current state</li>\n<li>Perform computation or side-effects</li>\n<li>Return an updated state</li>\n</ul>\n<h3 id=\"3-edges\">3. Edges</h3>\n<span class=\"paragraph\">Functions that determine which node to execute next:</span>\n<ul>\n<li><strong>Normal edges</strong>: Always go to a specific node</li>\n<li><strong>Conditional edges</strong>: Choose the next node based on state</li>\n</ul>\n<span class=\"paragraph\"><strong>\ud83d\udcda Documentation:</strong></span>\n<ul>\n<li><a href=\"https://langchain-ai.github.io/langgraph/concepts/low_level/\" rel=\"noopener noreferrer\" target=\"_blank\">LangGraph Low-Level Concepts</a></li>\n<li><a href=\"https://langchain-ai.github.io/langgraph/tutorials/introduction/\" rel=\"noopener noreferrer\" target=\"_blank\">LangGraph Quickstart</a></li>\n<li><a href=\"https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.StateGraph\" rel=\"noopener noreferrer\" target=\"_blank\">StateGraph API Reference</a></li>\n</ul></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "Kclp",
      "code_hash": "1f86111d997de2e1a7810859bc1ba624",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"task-3-building-a-react-agent-from-scratch\">Task 3: Building a ReAct Agent from Scratch</h2>\n<span class=\"paragraph\">Now let's build something more sophisticated: a <strong>ReAct agent</strong> that can:</span>\n<ol>\n<li><strong>Reason</strong> about what to do</li>\n<li><strong>Act</strong> by calling tools</li>\n<li><strong>Observe</strong> results</li>\n<li><strong>Repeat</strong> until done</li>\n</ol>\n<span class=\"paragraph\">This is exactly what <code>create_agent</code> does under the hood. Let's build it ourselves!</span>\n<h3 id=\"the-agent-loop-architecture\">The Agent Loop Architecture</h3>\n<div class=\"language-gdscript codehilite\"><pre><span></span><code><span class=\"w\">                    </span><span class=\"err\">\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510</span>\n<span class=\"w\">                    </span><span class=\"err\">\u2502</span><span class=\"w\">    </span><span class=\"n\">START</span><span class=\"w\">     </span><span class=\"err\">\u2502</span>\n<span class=\"w\">                    </span><span class=\"err\">\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</span>\n<span class=\"w\">                           </span><span class=\"err\">\u2502</span>\n<span class=\"w\">                           </span><span class=\"err\">\u25bc</span>\n<span class=\"w\">                    </span><span class=\"err\">\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510</span>\n<span class=\"w\">             </span><span class=\"err\">\u250c\u2500\u2500\u2500\u2500\u2500\u25ba\u2502</span><span class=\"w\">    </span><span class=\"n\">agent</span><span class=\"w\">     </span><span class=\"err\">\u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510</span>\n<span class=\"w\">             </span><span class=\"err\">\u2502</span><span class=\"w\">      </span><span class=\"err\">\u2502</span><span class=\"w\">  </span><span class=\"p\">(</span><span class=\"n\">call</span><span class=\"w\"> </span><span class=\"n\">LLM</span><span class=\"p\">)</span><span class=\"w\">  </span><span class=\"err\">\u2502</span><span class=\"w\">         </span><span class=\"err\">\u2502</span>\n<span class=\"w\">             </span><span class=\"err\">\u2502</span><span class=\"w\">      </span><span class=\"err\">\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</span><span class=\"w\">         </span><span class=\"err\">\u2502</span>\n<span class=\"w\">             </span><span class=\"err\">\u2502</span><span class=\"w\">             </span><span class=\"err\">\u2502</span><span class=\"w\">                 </span><span class=\"err\">\u2502</span>\n<span class=\"w\">             </span><span class=\"err\">\u2502</span><span class=\"w\">             </span><span class=\"err\">\u25bc</span><span class=\"w\">                 </span><span class=\"err\">\u2502</span>\n<span class=\"w\">             </span><span class=\"err\">\u2502</span><span class=\"w\">      </span><span class=\"err\">\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510</span><span class=\"w\">         </span><span class=\"err\">\u2502</span>\n<span class=\"w\">             </span><span class=\"err\">\u2502</span><span class=\"w\">      </span><span class=\"err\">\u2502</span><span class=\"w\"> </span><span class=\"n\">should_</span><span class=\"w\">      </span><span class=\"err\">\u2502</span><span class=\"w\">         </span><span class=\"err\">\u2502</span>\n<span class=\"w\">             </span><span class=\"err\">\u2502</span><span class=\"w\">      </span><span class=\"err\">\u2502</span><span class=\"w\"> </span><span class=\"k\">continue</span><span class=\"err\">?</span><span class=\"w\">    </span><span class=\"err\">\u2502</span><span class=\"w\">         </span><span class=\"err\">\u2502</span>\n<span class=\"w\">             </span><span class=\"err\">\u2502</span><span class=\"w\">      </span><span class=\"err\">\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</span><span class=\"w\">         </span><span class=\"err\">\u2502</span>\n<span class=\"w\">             </span><span class=\"err\">\u2502</span><span class=\"w\">             </span><span class=\"err\">\u2502</span><span class=\"w\">                 </span><span class=\"err\">\u2502</span>\n<span class=\"w\">             </span><span class=\"err\">\u2502</span><span class=\"w\">    </span><span class=\"n\">tool_calls</span><span class=\"err\">?</span><span class=\"w\">                </span><span class=\"err\">\u2502</span>\n<span class=\"w\">             </span><span class=\"err\">\u2502</span><span class=\"w\">     </span><span class=\"err\">\u2502</span><span class=\"w\">           </span><span class=\"err\">\u2502</span><span class=\"w\">             </span><span class=\"err\">\u2502</span>\n<span class=\"w\">             </span><span class=\"err\">\u2502</span><span class=\"w\">    </span><span class=\"n\">YES</span><span class=\"w\">         </span><span class=\"n\">NO</span><span class=\"w\">             </span><span class=\"err\">\u2502</span>\n<span class=\"w\">             </span><span class=\"err\">\u2502</span><span class=\"w\">     </span><span class=\"err\">\u2502</span><span class=\"w\">           </span><span class=\"err\">\u2502</span><span class=\"w\">             </span><span class=\"err\">\u2502</span>\n<span class=\"w\">             </span><span class=\"err\">\u2502</span><span class=\"w\">     </span><span class=\"err\">\u25bc</span><span class=\"w\">           </span><span class=\"err\">\u25bc</span><span class=\"w\">             </span><span class=\"err\">\u2502</span>\n<span class=\"w\">             </span><span class=\"err\">\u2502</span><span class=\"w\"> </span><span class=\"err\">\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510</span><span class=\"w\">  </span><span class=\"err\">\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510</span><span class=\"w\">         </span><span class=\"err\">\u2502</span>\n<span class=\"w\">             </span><span class=\"err\">\u2502</span><span class=\"w\"> </span><span class=\"err\">\u2502</span><span class=\"w\"> </span><span class=\"n\">tools</span><span class=\"w\">  </span><span class=\"err\">\u2502</span><span class=\"w\">  </span><span class=\"err\">\u2502</span><span class=\"w\">  </span><span class=\"n\">END</span><span class=\"w\">  </span><span class=\"err\">\u2502</span><span class=\"w\">         </span><span class=\"err\">\u2502</span>\n<span class=\"w\">             </span><span class=\"err\">\u2514\u2500\u2524</span><span class=\"p\">(</span><span class=\"n\">execute</span><span class=\"err\">\u2502</span><span class=\"w\">  </span><span class=\"err\">\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</span><span class=\"w\">         </span><span class=\"err\">\u2502</span>\n<span class=\"w\">               </span><span class=\"err\">\u2502</span><span class=\"w\"> </span><span class=\"n\">tools</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"err\">\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</span>\n<span class=\"w\">               </span><span class=\"err\">\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</span>\n</code></pre></div>\n<span class=\"paragraph\"><strong>\ud83d\udcda Documentation:</strong></span>\n<ul>\n<li><a href=\"https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/\" rel=\"noopener noreferrer\" target=\"_blank\">How to create a ReAct agent from scratch</a></li>\n<li><a href=\"https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#react-agent\" rel=\"noopener noreferrer\" target=\"_blank\">ReAct Agent Conceptual Guide</a></li>\n</ul></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "nWHF",
      "code_hash": "758ee39a93d6df047688860b0e518b58",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"task-4-adding-tools-to-your-agent\">Task 4: Adding Tools to Your Agent</h2>\n<span class=\"paragraph\">Tools are functions that the agent can call. We use the <code>@tool</code> decorator and <strong>bind</strong> them to the LLM.</span>\n<span class=\"paragraph\"><strong>\ud83d\udcda Documentation:</strong></span>\n<ul>\n<li><a href=\"https://python.langchain.com/docs/concepts/tools/\" rel=\"noopener noreferrer\" target=\"_blank\">LangChain Tools Conceptual Guide</a></li>\n<li><a href=\"https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html\" rel=\"noopener noreferrer\" target=\"_blank\">@tool Decorator Reference</a></li>\n<li><a href=\"https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode\" rel=\"noopener noreferrer\" target=\"_blank\">ToolNode Prebuilt</a></li>\n</ul></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "Pvdt",
      "code_hash": "e73fc00ebd5cb3acf9c3d450356f398e",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><hr />\n<h2 id=\"question-1\">\u2753 Question #1:</h2>\n<span class=\"paragraph\">In our from-scratch agent, we defined a <code>should_continue</code> function that returns either <code>\"tools\"</code> or <code>\"end\"</code>. How does this compare to how <code>create_agent</code> handles the same decision? What additional logic might <code>create_agent</code> include that we didn't implement?</span>\n<h5 id=\"answer\">Answer:</h5>\n<span class=\"paragraph\"><em>Both check for tool calls to decide whether to execute tools or end, but <code>create_agent</code> wraps this in a more robust implementation that handles edge cases we naive builders didn't bother thinking about.</em></span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "ZBYS",
      "code_hash": "0c0b7fa333317ac659d1b17fb6253e60",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"question-2\">\u2753 Question #2:</h2>\n<span class=\"paragraph\">We used <code>ToolNode</code> from <code>langgraph.prebuilt</code> to execute tools. Looking at the tool execution flow, what would happen if we wanted to add logging, error handling, or rate limiting to tool execution? How would building our own tool node give us more control?</span>\n<h5 id=\"answer\">Answer:</h5>\n<span class=\"paragraph\">Building your own tool node lets you wrap tool calls with logging, error handling, and rate limiting middleware - capabilities ToolNode keeps locked away.</span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "aLJB",
      "code_hash": "e67511de31f9c9a26fe3afa1b8bf436f",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><hr />\n<h2 id=\"activity-1-implement-a-custom-routing-function\">\ud83c\udfd7\ufe0f Activity #1: Implement a Custom Routing Function</h2>\n<span class=\"paragraph\">Extend the agent by implementing a <strong>custom routing function</strong> that adds more sophisticated logic.</span>\n<span class=\"paragraph\">Ideas:</span>\n<ul>\n<li>Add a maximum iteration limit to prevent infinite loops</li>\n<li>Route to different nodes based on the type of tool being called</li>\n<li>Add a \"thinking\" step before tool execution</li>\n</ul>\n<span class=\"paragraph\">Requirements:</span>\n<ol>\n<li>Modify the <code>should_continue</code> function or create a new one</li>\n<li>Add any new nodes if needed</li>\n<li>Rebuild and test the agent</li>\n</ol>\n<span class=\"paragraph\"><strong>\ud83d\udcda Documentation:</strong></span>\n<ul>\n<li><a href=\"https://langchain-ai.github.io/langgraph/concepts/low_level/#conditional-edges\" rel=\"noopener noreferrer\" target=\"_blank\">Conditional Edges</a></li>\n<li><a href=\"https://langchain-ai.github.io/langgraph/how-tos/branching/\" rel=\"noopener noreferrer\" target=\"_blank\">How to create branches for parallel node execution</a></li>\n</ul></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "AjVT",
      "code_hash": "7fb2602176b305f2abd163ca0122cf25",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><hr />\n<h1 id=\"breakout-room-2\">Breakout Room #2</h1>\n<h2 id=\"agentic-rag-with-local-models\">Agentic RAG with Local Models</h2></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "pHFh",
      "code_hash": "1bd467c8ac367fab0ac6da9d720ec3be",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><span class=\"paragraph\">Now let's build a full <strong>Agentic RAG</strong> system from scratch using our local models!</span>\n<span class=\"paragraph\">We'll transition from the <code>aimakerspace</code> utilities to the <strong>LangChain ecosystem</strong>:</span>\n<table>\n<thead>\n<tr>\n<th>Task</th>\n<th>aimakerspace</th>\n<th>LangChain</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Load Documents</td>\n<td><code>TextFileLoader</code></td>\n<td><code>TextLoader</code></td>\n</tr>\n<tr>\n<td>Split Text</td>\n<td><code>CharacterTextSplitter</code></td>\n<td><code>RecursiveCharacterTextSplitter</code></td>\n</tr>\n<tr>\n<td>Embeddings</td>\n<td>Custom</td>\n<td><code>OllamaEmbeddings</code></td>\n</tr>\n</tbody>\n</table></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "NCOB",
      "code_hash": "321eb303a600c856956ca9713ac22f05",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"task-5-loading-chunking-with-langchain\">Task 5: Loading &amp; Chunking with LangChain</h2>\n<span class=\"paragraph\">Let's use LangChain's document loaders and text splitters.</span>\n<span class=\"paragraph\"><strong>\ud83d\udcda Documentation:</strong></span>\n<ul>\n<li><a href=\"https://python.langchain.com/docs/concepts/document_loaders/\" rel=\"noopener noreferrer\" target=\"_blank\">Document Loaders Conceptual Guide</a></li>\n<li><a href=\"https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.text.TextLoader.html\" rel=\"noopener noreferrer\" target=\"_blank\">TextLoader Reference</a></li>\n<li><a href=\"https://python.langchain.com/docs/how_to/recursive_text_splitter/\" rel=\"noopener noreferrer\" target=\"_blank\">RecursiveCharacterTextSplitter</a></li>\n<li><a href=\"https://python.langchain.com/docs/concepts/text_splitters/\" rel=\"noopener noreferrer\" target=\"_blank\">Text Splitters Conceptual Guide</a></li>\n</ul></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "TXez",
      "code_hash": "2765948fbcd4ddcad4c21cbb400a9509",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"task-6-setting-up-qdrant-with-local-embeddings\">Task 6: Setting up Qdrant with Local Embeddings</h2>\n<span class=\"paragraph\">Now we'll use <strong>OllamaEmbeddings</strong> with the <code>embeddinggemma</code> model - completely local!</span>\n<span class=\"paragraph\"><strong>\ud83d\udcda Documentation:</strong></span>\n<ul>\n<li><a href=\"https://python.langchain.com/api_reference/ollama/embeddings/langchain_ollama.embeddings.OllamaEmbeddings.html\" rel=\"noopener noreferrer\" target=\"_blank\">OllamaEmbeddings Reference</a></li>\n<li><a href=\"https://python.langchain.com/docs/integrations/vectorstores/qdrant/\" rel=\"noopener noreferrer\" target=\"_blank\">Qdrant Vector Store Integration</a></li>\n<li><a href=\"https://python.langchain.com/docs/concepts/embedding_models/\" rel=\"noopener noreferrer\" target=\"_blank\">Embedding Models Conceptual Guide</a></li>\n<li><a href=\"https://ai.google.dev/gemma/docs/embeddinggemma\" rel=\"noopener noreferrer\" target=\"_blank\">EmbeddingGemma Overview (Google)</a></li>\n</ul></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "wAgl",
      "code_hash": "a13a4e67696ade52eba65c5017f476b7",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"task-7-creating-a-rag-tool\">Task 7: Creating a RAG Tool</h2>\n<span class=\"paragraph\">Now let's wrap our retriever as a tool that the agent can use.</span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "dGlV",
      "code_hash": "8364a51e600aae71d9e848a2054828b4",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"task-8-building-agentic-rag-from-scratch\">Task 8: Building Agentic RAG from Scratch</h2>\n<span class=\"paragraph\">Now let's put it all together - a complete agentic RAG system built from scratch!</span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "mWxS",
      "code_hash": "d7f251ece757cad0c92593f234c4a707",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><hr />\n<h2 id=\"question-3\">\u2753 Question #3:</h2>\n<span class=\"paragraph\">Compare the experience of building an agent from scratch with LangGraph versus using <code>create_agent</code> from Session 3. What are the trade-offs between control and convenience? When would you choose one approach over the other?</span>\n<h5 id=\"answer\">Answer:</h5>\n<span class=\"paragraph\"><em>Building from scratch gives you complete control and visibility but requires writing every component yourself, while <code>create_agent</code> gets you running quickly with the convenient trade-off that debugging becomes an exercise in guessing what's happening behind the abstraction.</em></span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "CcZR",
      "code_hash": "a97dc68211e1816ade754f919f7eb615",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"question-4\">\u2753 Question #4:</h2>\n<span class=\"paragraph\">We used local models (gpt-oss:20b and embeddinggemma) instead of cloud APIs. What are the advantages and disadvantages of this approach?</span>\n<h5 id=\"answer\">Answer:</h5>\n<span class=\"paragraph\"><em>Local models keep your data private and avoid API costs, though you'll pay for it with slower inference and the hardware requirements that make your laptop sound like a jet engine.</em></span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "YWSi",
      "code_hash": "edf765ad61236bdae87b97e6271b909d",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><hr />\n<h2 id=\"activity-2-extend-the-agent-with-memory\">\ud83c\udfd7\ufe0f Activity #2: Extend the Agent with Memory</h2>\n<span class=\"paragraph\">LangGraph supports <strong>checkpointing</strong> which enables conversation memory across invocations.</span>\n<span class=\"paragraph\">Your task: Add memory to the RAG agent so it can:</span>\n<ol>\n<li>Remember previous questions in the conversation</li>\n<li>Reference past context when answering new questions</li>\n<li>Build on previous answers</li>\n</ol>\n<span class=\"paragraph\">Hint: Use <code>MemorySaver</code> from <code>langgraph.checkpoint.memory</code> and pass a <code>thread_id</code> in the config.</span>\n<span class=\"paragraph\"><strong>\ud83d\udcda Documentation:</strong></span>\n<ul>\n<li><a href=\"https://langchain-ai.github.io/langgraph/concepts/persistence/\" rel=\"noopener noreferrer\" target=\"_blank\">LangGraph Persistence &amp; Memory</a></li>\n<li><a href=\"https://langchain-ai.github.io/langgraph/how-tos/persistence/\" rel=\"noopener noreferrer\" target=\"_blank\">How to add memory to your graph</a></li>\n<li><a href=\"https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver\" rel=\"noopener noreferrer\" target=\"_blank\">MemorySaver Reference</a></li>\n</ul></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "xvXZ",
      "code_hash": "6f087db6a2c1cb6407a9e2ff4621eb97",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><hr />\n<h2 id=\"summary\">Summary</h2>\n<span class=\"paragraph\">In this session, we:</span>\n<ol>\n<li><strong>Built agents from scratch</strong> using LangGraph's low-level primitives (StateGraph, nodes, edges)</li>\n<li><strong>Used local open-source models</strong> with Ollama (gpt-oss:20b + embeddinggemma)</li>\n<li><strong>Transitioned to LangChain</strong> for document loading and text splitting</li>\n<li><strong>Created an Agentic RAG system</strong> that intelligently decides when to retrieve information</li>\n</ol>\n<h3 id=\"key-takeaways\">Key Takeaways:</h3>\n<ul>\n<li><strong>StateGraph</strong> gives you full control over agent architecture</li>\n<li><strong>Conditional edges</strong> enable dynamic routing based on LLM decisions</li>\n<li><strong>Local models</strong> provide privacy and cost savings, with trade-offs in performance</li>\n<li><strong>LangSmith</strong> provides crucial visibility regardless of where your models run</li>\n</ul>\n<h3 id=\"whats-next\">What's Next?</h3>\n<span class=\"paragraph\">Now that you understand the fundamentals, you can:</span>\n<ul>\n<li>Add more sophisticated routing logic</li>\n<li>Implement human-in-the-loop patterns</li>\n<li>Build multi-agent systems</li>\n<li>Deploy to production with LangGraph Platform</li>\n</ul>\n<span class=\"paragraph\"><strong>\ud83d\udcda Further Reading:</strong></span>\n<ul>\n<li><a href=\"https://langchain-ai.github.io/langgraph/how-tos/\" rel=\"noopener noreferrer\" target=\"_blank\">LangGraph How-To Guides</a></li>\n<li><a href=\"https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/\" rel=\"noopener noreferrer\" target=\"_blank\">Human-in-the-Loop Patterns</a></li>\n<li><a href=\"https://langchain-ai.github.io/langgraph/concepts/multi_agent/\" rel=\"noopener noreferrer\" target=\"_blank\">Multi-Agent Architectures</a></li>\n<li><a href=\"https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/\" rel=\"noopener noreferrer\" target=\"_blank\">LangGraph Platform</a></li>\n</ul></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "Hbol",
      "code_hash": "1d0db38904205bec4d6f6f6a1f6cec3e",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "lEQa",
      "code_hash": "6e3d6df0d702d1e02604d091f82f9107",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "PKri",
      "code_hash": "0fcea2b386548740dbe82f48c35bf35a",
      "outputs": [],
      "console": []
    },
    {
      "id": "SFPL",
      "code_hash": "a6daa97597e5a07e8fb7dd145ae567e3",
      "outputs": [],
      "console": []
    },
    {
      "id": "BYtC",
      "code_hash": "61347dd72b88b2b2e51a057d8257df7c",
      "outputs": [],
      "console": []
    },
    {
      "id": "RGSE",
      "code_hash": "e56685655d5ebe4c5e5e04126304d95c",
      "outputs": [],
      "console": []
    },
    {
      "id": "emfo",
      "code_hash": "5fc0aa6968349e8225c5face1206b7e4",
      "outputs": [],
      "console": []
    },
    {
      "id": "Hstk",
      "code_hash": "cfcb4bead5c7cdf28f0870274515a341",
      "outputs": [],
      "console": []
    },
    {
      "id": "iLit",
      "code_hash": "5c9dfd9f9cfb5e2ea99dfc94cadf5899",
      "outputs": [],
      "console": []
    },
    {
      "id": "ZHCJ",
      "code_hash": "8a949e34a1d9c44269116327459a3b6b",
      "outputs": [],
      "console": []
    },
    {
      "id": "ROlb",
      "code_hash": "49a118d245122876c363a4aeaf7f8a93",
      "outputs": [],
      "console": []
    },
    {
      "id": "qnkX",
      "code_hash": "63e87c92fcd993bb2203e5da60f83ad2",
      "outputs": [],
      "console": []
    },
    {
      "id": "TqIu",
      "code_hash": "4f35184354759ef91bbf574c05b9d174",
      "outputs": [],
      "console": []
    },
    {
      "id": "Vxnm",
      "code_hash": "1e3744df1de831e1210198aee445c5b8",
      "outputs": [],
      "console": []
    },
    {
      "id": "DnEU",
      "code_hash": "92da74f8e328b208179d183e43a2d614",
      "outputs": [],
      "console": []
    },
    {
      "id": "ulZA",
      "code_hash": "e1beb27d84bf2db90fee02a2c937cfa1",
      "outputs": [],
      "console": []
    },
    {
      "id": "ecfG",
      "code_hash": "8aeba28d609dbff98666a93d298e4d1a",
      "outputs": [],
      "console": []
    },
    {
      "id": "nHfw",
      "code_hash": "e653fac7730def90f3e575cb8dcacf6c",
      "outputs": [],
      "console": []
    },
    {
      "id": "xXTn",
      "code_hash": "6d5d19ebd95e193b64111bc9d2274c6a",
      "outputs": [],
      "console": []
    },
    {
      "id": "aqbW",
      "code_hash": "9bebfd5fc55a9208dd8f55202db30d0e",
      "outputs": [],
      "console": []
    },
    {
      "id": "TRpd",
      "code_hash": "75fa0ae25399920460d7a51a9708c6d2",
      "outputs": [],
      "console": []
    },
    {
      "id": "dNNg",
      "code_hash": "88176e46065e9f9c9ae7a52c46bc71da",
      "outputs": [],
      "console": []
    },
    {
      "id": "yCnT",
      "code_hash": "667fe0cb6f4a3fabed640ef6d5fd8c1f",
      "outputs": [],
      "console": []
    },
    {
      "id": "wlCL",
      "code_hash": "7da9700f9ee3d01a1630cd7d44c759d2",
      "outputs": [],
      "console": []
    },
    {
      "id": "kqZH",
      "code_hash": "efbad3abbc945ca9869ae38e72236b4d",
      "outputs": [],
      "console": []
    },
    {
      "id": "rEll",
      "code_hash": "c68827a85674a27e883cbe48fedd09e9",
      "outputs": [],
      "console": []
    },
    {
      "id": "SdmI",
      "code_hash": "ceae7e5da34c2aa5b05d5b293be5f15d",
      "outputs": [],
      "console": []
    },
    {
      "id": "lgWD",
      "code_hash": "cc44fa3fdb135f4108dde5c47ace3846",
      "outputs": [],
      "console": []
    },
    {
      "id": "yOPj",
      "code_hash": "04beba2f5ef5b1cc5719f60fc8d5c9e1",
      "outputs": [],
      "console": []
    },
    {
      "id": "fwwy",
      "code_hash": "20030fca1cde01f72f19709203fa2d85",
      "outputs": [],
      "console": []
    },
    {
      "id": "LJZf",
      "code_hash": "0cce0b1caf73712121f5c239236ed41b",
      "outputs": [],
      "console": []
    },
    {
      "id": "urSm",
      "code_hash": "7aabcb7508bfc0a1bdfd1b9e7cdf3af7",
      "outputs": [],
      "console": []
    },
    {
      "id": "jxvo",
      "code_hash": "bc13246a401754e2679b9dfa6e7ab984",
      "outputs": [],
      "console": []
    },
    {
      "id": "zlud",
      "code_hash": "8b5e3ac55bdbc55f60237e6206bccf08",
      "outputs": [],
      "console": []
    },
    {
      "id": "tZnO",
      "code_hash": "542e7b18a6afb1fa34a8da9768f3f964",
      "outputs": [],
      "console": []
    }
  ]
}